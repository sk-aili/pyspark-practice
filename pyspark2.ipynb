{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#**Skill: PySpark**\n","If your agenda is one of the following, please follow along this notebook to learn PySpark skill:\n","- Want to learn PySpark? \n","- Have a huge dataset and want to do analysis to extract insights from it?\n","- Want to become a Data Engineer?\n","\n","**Background**:   \n","[PySpark](https://spark.apache.org/docs/latest/api/python/) is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.    \n","\n","In this notebook, we will try to answer few questions to learn PySpark. Let's practice!"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![Unknown.png](data:image/png;base64,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)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First, we will get data, employees.csv file from the [link](https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv).   \n","The below command will get data and save it in _data folder"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1674146318834,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"BPl1Ud3s5ElN","outputId":"0f998a61-dbde-40aa-f01a-f8f3de76d534"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-02-02 14:26:23--  https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\n","Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3778 (3.7K) [text/plain]\n","Saving to: ‘/workspaces/pyspark-practice/_data/employees.csv’\n","\n","/workspaces/pyspark 100%[===================>]   3.69K  --.-KB/s    in 0s      \n","\n","2023-02-02 14:26:23 (44.8 MB/s) - ‘/workspaces/pyspark-practice/_data/employees.csv’ saved [3778/3778]\n","\n"]}],"source":["#!wget \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\" -O /workspaces/pyspark-practice/_data/employees.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Install PySpark in this notebook"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37755,"status":"ok","timestamp":1674146444686,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"B_6nV_oH5rlI","outputId":"41c3c1b6-88ca-4bf2-e27d-5823d6c644e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=268f04df7b060df0c9d8ee6b8d4679bdfeb2265d98ae1b5b6baf04ad3e06a98e\n","  Stored in directory: /home/codespace/.cache/pip/wheels/9c/aa/b1/8433fd8b1afe7eb31196cc74a42cd778bcb52636a428da079d\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install pyspark"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Introduction\n","[Apache Spark](https://sparkbyexamples.com/pyspark/what-is-pyspark-and-who-uses-it/) is an open-source framework written in Scala for processing large datasets in a distributed manner (in a cluster). Spark runs 100 times faster than traditional processing due to its in-memory processing. PySpark is a Python API for Apache Spark to process larger datasets in a distributed cluster. It is written in Python to run a Python application using Apache Spark capabilities. \n","\n","[The entry point](https://spark.apache.org/docs/latest/sql-getting-started.html) into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder. "]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7891,"status":"ok","timestamp":1674146461524,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"Fr-fu9Xs5Swp"},"outputs":[{"name":"stdout","output_type":"stream","text":["23/02/02 14:36:07 WARN Utils: Your hostname, codespaces-53d0de resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n","23/02/02 14:36:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["23/02/02 14:36:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The below command is used to read the csv dataset with header  *header = True*.    \n","*inferSchema = True* defines the datatype of a column and we don't have to explicitly define schema of the dataset. "]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8424,"status":"ok","timestamp":1674146562420,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"PlgkPRkr5qdk","outputId":"b4dc6fff-ffd5-44e1-9d25-170a0631af21"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n","|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n","+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n","|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n","|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n","|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n","|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n","|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n","|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n","|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n","|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n","|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n","|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n","|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|\n","|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|\n","|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|\n","|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|\n","|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|\n","|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|\n","|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|\n","|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|\n","|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|\n","|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|\n","+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["df = spark.read.csv(\"_data/employees.csv\", header=True, inferSchema=True)\n","df.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's check how the spark has defined the datatype of columns."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1674147490607,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"HXuc8YMu911d","outputId":"b1abca17-25f8-4493-87a8-f1d89918d12a"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- EMPLOYEE_ID: integer (nullable = true)\n"," |-- FIRST_NAME: string (nullable = true)\n"," |-- LAST_NAME: string (nullable = true)\n"," |-- EMAIL: string (nullable = true)\n"," |-- PHONE_NUMBER: string (nullable = true)\n"," |-- HIRE_DATE: string (nullable = true)\n"," |-- JOB_ID: string (nullable = true)\n"," |-- SALARY: integer (nullable = true)\n"," |-- COMMISSION_PCT: string (nullable = true)\n"," |-- MANAGER_ID: string (nullable = true)\n"," |-- DEPARTMENT_ID: integer (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fx87zz876cj1"},"source":["**Business Requirement 1: Take “EMPLOYEE_ID” “FIRST_NAME” to new Data Frame along with “JOINING_DATE” (in yyyy-MM-dd ) same as “HIRE_DATE”**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this question, we are trying to select three columns(employee_id, first_name, hire_date) and format hire_date in yyyy-MM-dd format and alias it as Joining_date"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1674148037317,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"9OtjZpWI6R9u","outputId":"f06ba885-7554-4db1-f966-337f682e690b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|\n","+-----------+----------+------------+\n","|        198|    Donald|  2007-06-21|\n","|        199|   Douglas|  2008-01-13|\n","|        200|  Jennifer|  2003-09-17|\n","|        201|   Michael|  2004-02-17|\n","|        202|       Pat|  2005-08-17|\n","|        203|     Susan|  2002-06-07|\n","|        204|   Hermann|  2002-06-07|\n","|        205|   Shelley|  2002-06-07|\n","|        206|   William|  2002-06-07|\n","|        100|    Steven|  2003-06-17|\n","|        101|     Neena|  2005-09-21|\n","|        102|       Lex|  2001-01-13|\n","|        103| Alexander|  2006-01-03|\n","|        104|     Bruce|  2007-05-21|\n","|        105|     David|  2005-06-25|\n","|        106|     Valli|  2006-02-05|\n","|        107|     Diana|  2007-02-07|\n","|        108|     Nancy|  2002-08-17|\n","|        109|    Daniel|  2002-08-16|\n","|        110|      John|  2005-09-28|\n","+-----------+----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import date_format, to_date\n","df_joining_date = df.select([\"EMPLOYEE_ID\", \"FIRST_NAME\", \\\n","           date_format(to_date(df.HIRE_DATE, \"dd-MMM-yy\"), \"yyyy-MM-dd\").alias(\"JOINING_DATE\")])\n","\n","df_joining_date.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CZv2M6qo-eBS"},"source":["**Business Requirement 2: Print the experience of employees in terms of days based on employee's joining date along with employee_id, first_name and their joining_date**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Based on our business requirement, we have to retrieve experience of employees in days and we are going to achieve this by taking difference between current_date and their joining date.    \n","We will use [datediff()](https://sparkbyexamples.com/pyspark/pyspark-difference-between-two-dates-days-months-years/) and [current_date()](https://sparkbyexamples.com/pyspark/pyspark-current-date-timestamp/) functions from PySpark SQL library."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":591,"status":"ok","timestamp":1674148253147,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"bKmXH4AM9VUY","outputId":"ace76b5a-445f-47c4-dd6d-fb250ae2c252"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+-----------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|Exp_in_days|\n","+-----------+----------+------------+-----------+\n","|        198|    Donald|  2007-06-21|       5705|\n","|        199|   Douglas|  2008-01-13|       5499|\n","|        200|  Jennifer|  2003-09-17|       7078|\n","|        201|   Michael|  2004-02-17|       6925|\n","|        202|       Pat|  2005-08-17|       6378|\n","|        203|     Susan|  2002-06-07|       7545|\n","|        204|   Hermann|  2002-06-07|       7545|\n","|        205|   Shelley|  2002-06-07|       7545|\n","|        206|   William|  2002-06-07|       7545|\n","|        100|    Steven|  2003-06-17|       7170|\n","|        101|     Neena|  2005-09-21|       6343|\n","|        102|       Lex|  2001-01-13|       8055|\n","|        103| Alexander|  2006-01-03|       6239|\n","|        104|     Bruce|  2007-05-21|       5736|\n","|        105|     David|  2005-06-25|       6431|\n","|        106|     Valli|  2006-02-05|       6206|\n","|        107|     Diana|  2007-02-07|       5839|\n","|        108|     Nancy|  2002-08-17|       7474|\n","|        109|    Daniel|  2002-08-16|       7475|\n","|        110|      John|  2005-09-28|       6336|\n","+-----------+----------+------------+-----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import datediff, current_date\n","\n","datediff_df = df_joining_date.withColumn(\"Exp_in_days\", datediff(current_date(), df_joining_date.JOINING_DATE))\n","datediff_df.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jJD4We2iAgJH"},"source":["**Business Requirement 3: Get the employees work experience in months and print FIRST_NAME|EMPLOYEE_ID|JOINING_DATE|Exp_in_months**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Similar to the datediff() function, we have months_between() function in PySpark SQL library where it will give us the number of months in between two dates."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1674148348043,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"f1N2ML8f_sgm","outputId":"5793360d-1a28-4746-daa6-5347b795bf8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+-------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|Exp_in_months|\n","+-----------+----------+------------+-------------+\n","|        198|    Donald|  2007-06-21| 187.38709677|\n","|        199|   Douglas|  2008-01-13| 180.64516129|\n","|        200|  Jennifer|  2003-09-17| 232.51612903|\n","|        201|   Michael|  2004-02-17| 227.51612903|\n","|        202|       Pat|  2005-08-17| 209.51612903|\n","|        203|     Susan|  2002-06-07| 247.83870968|\n","|        204|   Hermann|  2002-06-07| 247.83870968|\n","|        205|   Shelley|  2002-06-07| 247.83870968|\n","|        206|   William|  2002-06-07| 247.83870968|\n","|        100|    Steven|  2003-06-17| 235.51612903|\n","|        101|     Neena|  2005-09-21| 208.38709677|\n","|        102|       Lex|  2001-01-13| 264.64516129|\n","|        103| Alexander|  2006-01-03| 204.96774194|\n","|        104|     Bruce|  2007-05-21| 188.38709677|\n","|        105|     David|  2005-06-25| 211.25806452|\n","|        106|     Valli|  2006-02-05| 203.90322581|\n","|        107|     Diana|  2007-02-07| 191.83870968|\n","|        108|     Nancy|  2002-08-17| 245.51612903|\n","|        109|    Daniel|  2002-08-16|  245.5483871|\n","|        110|      John|  2005-09-28| 208.16129032|\n","+-----------+----------+------------+-------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import months_between, current_date\n","\n","monthdiff_df = df_joining_date.withColumn(\"Exp_in_months\", months_between(current_date(), df_joining_date.JOINING_DATE))\n","monthdiff_df.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pE1FBMR7BL_E"},"source":["**Business Requirement 4: By policy, employee will be promoted after 20 months of their joining. Get the employees promotion date and print their FIRST_NAME|EMPLOYEE_ID|JOINING_DATE|Promotion Date**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Spark SQL provides DataFrame function [add_months()](https://sparkbyexamples.com/spark/spark-functions-adding-days-months-year/) to add or subtract months from a Date Column"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1674148602350,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"d5zQU_k-BC_A","outputId":"d36ff27b-e956-439e-c149-d28072fe8e8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+--------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|PROMOTION_DATE|\n","+-----------+----------+------------+--------------+\n","|        198|    Donald|  2007-06-21|    2009-02-21|\n","|        199|   Douglas|  2008-01-13|    2009-09-13|\n","|        200|  Jennifer|  2003-09-17|    2005-05-17|\n","|        201|   Michael|  2004-02-17|    2005-10-17|\n","|        202|       Pat|  2005-08-17|    2007-04-17|\n","|        203|     Susan|  2002-06-07|    2004-02-07|\n","|        204|   Hermann|  2002-06-07|    2004-02-07|\n","|        205|   Shelley|  2002-06-07|    2004-02-07|\n","|        206|   William|  2002-06-07|    2004-02-07|\n","|        100|    Steven|  2003-06-17|    2005-02-17|\n","|        101|     Neena|  2005-09-21|    2007-05-21|\n","|        102|       Lex|  2001-01-13|    2002-09-13|\n","|        103| Alexander|  2006-01-03|    2007-09-03|\n","|        104|     Bruce|  2007-05-21|    2009-01-21|\n","|        105|     David|  2005-06-25|    2007-02-25|\n","|        106|     Valli|  2006-02-05|    2007-10-05|\n","|        107|     Diana|  2007-02-07|    2008-10-07|\n","|        108|     Nancy|  2002-08-17|    2004-04-17|\n","|        109|    Daniel|  2002-08-16|    2004-04-16|\n","|        110|      John|  2005-09-28|    2007-05-28|\n","+-----------+----------+------------+--------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import add_months\n","\n","promotion_df = df_joining_date.withColumn(\"PROMOTION_DATE\", \\\n","                                          add_months(df_joining_date.JOINING_DATE, 20)\n","                                          )\n","\n","promotion_df.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gb-YftO6CN91"},"source":["**Business Requirement 5: Employee will be onboarded to a project after 7 days of their joining. Get the employees project onboarding date and print their FIRST_NAME|EMPLOYEE_ID|JOINING_DATE|Project_OnBoarding**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Similar to the above add_months() function, we have date_add() to add number of days to a date column"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1674148731695,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"fhDY8RGGCF8L","outputId":"fd32fe90-aab9-4b56-8799-79cdc3c9ef9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+------------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|PROJECT_ONBOARDING|\n","+-----------+----------+------------+------------------+\n","|        198|    Donald|  2007-06-21|        2007-06-28|\n","|        199|   Douglas|  2008-01-13|        2008-01-20|\n","|        200|  Jennifer|  2003-09-17|        2003-09-24|\n","|        201|   Michael|  2004-02-17|        2004-02-24|\n","|        202|       Pat|  2005-08-17|        2005-08-24|\n","|        203|     Susan|  2002-06-07|        2002-06-14|\n","|        204|   Hermann|  2002-06-07|        2002-06-14|\n","|        205|   Shelley|  2002-06-07|        2002-06-14|\n","|        206|   William|  2002-06-07|        2002-06-14|\n","|        100|    Steven|  2003-06-17|        2003-06-24|\n","|        101|     Neena|  2005-09-21|        2005-09-28|\n","|        102|       Lex|  2001-01-13|        2001-01-20|\n","|        103| Alexander|  2006-01-03|        2006-01-10|\n","|        104|     Bruce|  2007-05-21|        2007-05-28|\n","|        105|     David|  2005-06-25|        2005-07-02|\n","|        106|     Valli|  2006-02-05|        2006-02-12|\n","|        107|     Diana|  2007-02-07|        2007-02-14|\n","|        108|     Nancy|  2002-08-17|        2002-08-24|\n","|        109|    Daniel|  2002-08-16|        2002-08-23|\n","|        110|      John|  2005-09-28|        2005-10-05|\n","+-----------+----------+------------+------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import date_add\n","\n","proj_onboarding_df = df_joining_date.withColumn(\"PROJECT_ONBOARDING\", \\\n","                                                date_add(df_joining_date.JOINING_DATE, 7))\n","\n","proj_onboarding_df.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WoVAhd_aCpUH"},"source":["**Business Requirement 6: Get the date on which employees received their offer letter. By policy, employees will receive their joining letter 15 days post to their offer letter date, and print their FIRST_NAME|EMPLOYEE_ID|JOINING_DATE|Offer_letter**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Similar to the above add_months() function, we have date_sub() to substract number of days from a date column"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1674148834975,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"ozDM3HVIClfq","outputId":"af4bcbbb-ddbe-430f-c218-695fffa11dbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|OFFER_LETTER|\n","+-----------+----------+------------+------------+\n","|        198|    Donald|  2007-06-21|  2007-06-06|\n","|        199|   Douglas|  2008-01-13|  2007-12-29|\n","|        200|  Jennifer|  2003-09-17|  2003-09-02|\n","|        201|   Michael|  2004-02-17|  2004-02-02|\n","|        202|       Pat|  2005-08-17|  2005-08-02|\n","|        203|     Susan|  2002-06-07|  2002-05-23|\n","|        204|   Hermann|  2002-06-07|  2002-05-23|\n","|        205|   Shelley|  2002-06-07|  2002-05-23|\n","|        206|   William|  2002-06-07|  2002-05-23|\n","|        100|    Steven|  2003-06-17|  2003-06-02|\n","|        101|     Neena|  2005-09-21|  2005-09-06|\n","|        102|       Lex|  2001-01-13|  2000-12-29|\n","|        103| Alexander|  2006-01-03|  2005-12-19|\n","|        104|     Bruce|  2007-05-21|  2007-05-06|\n","|        105|     David|  2005-06-25|  2005-06-10|\n","|        106|     Valli|  2006-02-05|  2006-01-21|\n","|        107|     Diana|  2007-02-07|  2007-01-23|\n","|        108|     Nancy|  2002-08-17|  2002-08-02|\n","|        109|    Daniel|  2002-08-16|  2002-08-01|\n","|        110|      John|  2005-09-28|  2005-09-13|\n","+-----------+----------+------------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import date_sub\n","\n","offer_letter_df = df_joining_date.withColumn(\"OFFER_LETTER\", \\\n","                                             date_sub(df_joining_date.JOINING_DATE, 15))\n","\n","offer_letter_df.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZwslpND8DCmS"},"source":["**Business Requirement 7: Get the year of employee’s joining date**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In the same library as above, we have [year()](https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/) function with will retrive the year from a date column yyyy-MM-dd"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1674148921159,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"03AKtInhC-vk","outputId":"044eac97-b762-448e-e33e-510509d6d780"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+---------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|YEAR_OF_JOINING|\n","+-----------+----------+------------+---------------+\n","|        198|    Donald|  2007-06-21|           2007|\n","|        199|   Douglas|  2008-01-13|           2008|\n","|        200|  Jennifer|  2003-09-17|           2003|\n","|        201|   Michael|  2004-02-17|           2004|\n","|        202|       Pat|  2005-08-17|           2005|\n","|        203|     Susan|  2002-06-07|           2002|\n","|        204|   Hermann|  2002-06-07|           2002|\n","|        205|   Shelley|  2002-06-07|           2002|\n","|        206|   William|  2002-06-07|           2002|\n","|        100|    Steven|  2003-06-17|           2003|\n","|        101|     Neena|  2005-09-21|           2005|\n","|        102|       Lex|  2001-01-13|           2001|\n","|        103| Alexander|  2006-01-03|           2006|\n","|        104|     Bruce|  2007-05-21|           2007|\n","|        105|     David|  2005-06-25|           2005|\n","|        106|     Valli|  2006-02-05|           2006|\n","|        107|     Diana|  2007-02-07|           2007|\n","|        108|     Nancy|  2002-08-17|           2002|\n","|        109|    Daniel|  2002-08-16|           2002|\n","|        110|      John|  2005-09-28|           2005|\n","+-----------+----------+------------+---------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import year\n","\n","year_of_joining_df = df_joining_date.withColumn(\"YEAR_OF_JOINING\", \\\n","                                                year(df_joining_date.JOINING_DATE))\n","\n","year_of_joining_df.show(20)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YWjfJl3vDjPt"},"source":["**Business Requirement 8: Get the month on which employees joined and get their FIRST_NAME|EMPLOYEE_ID|JOINING_DATE|month_of_joining**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Similar to year() function, we have month() function to retrieve month from a date column. We can print the month number and month name as well. Here's how to do it..."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1674149223557,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"O8Th-G_SDTwe","outputId":"61a8d151-4ca4-400c-be4e-198a48b26319"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+----------------+---------------------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|MONTH_OF_JOINING|MONTH_NAME_OF_JOINING|\n","+-----------+----------+------------+----------------+---------------------+\n","|        198|    Donald|  2007-06-21|               6|                  Jun|\n","|        199|   Douglas|  2008-01-13|               1|                  Jan|\n","|        200|  Jennifer|  2003-09-17|               9|                  Sep|\n","|        201|   Michael|  2004-02-17|               2|                  Feb|\n","|        202|       Pat|  2005-08-17|               8|                  Aug|\n","|        203|     Susan|  2002-06-07|               6|                  Jun|\n","|        204|   Hermann|  2002-06-07|               6|                  Jun|\n","|        205|   Shelley|  2002-06-07|               6|                  Jun|\n","|        206|   William|  2002-06-07|               6|                  Jun|\n","|        100|    Steven|  2003-06-17|               6|                  Jun|\n","|        101|     Neena|  2005-09-21|               9|                  Sep|\n","|        102|       Lex|  2001-01-13|               1|                  Jan|\n","|        103| Alexander|  2006-01-03|               1|                  Jan|\n","|        104|     Bruce|  2007-05-21|               5|                  May|\n","|        105|     David|  2005-06-25|               6|                  Jun|\n","|        106|     Valli|  2006-02-05|               2|                  Feb|\n","|        107|     Diana|  2007-02-07|               2|                  Feb|\n","|        108|     Nancy|  2002-08-17|               8|                  Aug|\n","|        109|    Daniel|  2002-08-16|               8|                  Aug|\n","|        110|      John|  2005-09-28|               9|                  Sep|\n","+-----------+----------+------------+----------------+---------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import month, date_format\n","\n","month_of_joining_df = df_joining_date.withColumn(\"MONTH_OF_JOINING\", \\\n","                                                 month(df_joining_date.JOINING_DATE)) \\\n","                                                 .withColumn(\"MONTH_NAME_OF_JOINING\", \\\n","                                                 date_format(df_joining_date.JOINING_DATE, \"MMM\"))\n","\n","month_of_joining_df.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SRUs38-_DZKR"},"source":["**Business Requirement 9: Project KT is being given on the immediate Monday of employee's joining date. Get the date on which employee's received their project KT and print FIRST_NAME|EMPLOYEE_ID|JOINING_DATE|Project_KT**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here's the tricky question. We could use [next_day()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.next_day.html) function from PySpark SQL library to return the first date which is later than the value of the date column. "]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1674149818433,"user":{"displayName":"Sai Kumar Aili","userId":"14781724337991853849"},"user_tz":300},"id":"uZOkGGcADi00","outputId":"36eeab94-2bba-4818-def2-1b8c04c2749f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+------------+----------+\n","|EMPLOYEE_ID|FIRST_NAME|JOINING_DATE|PROJECT_KT|\n","+-----------+----------+------------+----------+\n","|        198|    Donald|  2007-06-21|2007-06-25|\n","|        199|   Douglas|  2008-01-13|2008-01-14|\n","|        200|  Jennifer|  2003-09-17|2003-09-22|\n","|        201|   Michael|  2004-02-17|2004-02-23|\n","|        202|       Pat|  2005-08-17|2005-08-22|\n","|        203|     Susan|  2002-06-07|2002-06-10|\n","|        204|   Hermann|  2002-06-07|2002-06-10|\n","|        205|   Shelley|  2002-06-07|2002-06-10|\n","|        206|   William|  2002-06-07|2002-06-10|\n","|        100|    Steven|  2003-06-17|2003-06-23|\n","|        101|     Neena|  2005-09-21|2005-09-26|\n","|        102|       Lex|  2001-01-13|2001-01-15|\n","|        103| Alexander|  2006-01-03|2006-01-09|\n","|        104|     Bruce|  2007-05-21|2007-05-28|\n","|        105|     David|  2005-06-25|2005-06-27|\n","|        106|     Valli|  2006-02-05|2006-02-06|\n","|        107|     Diana|  2007-02-07|2007-02-12|\n","|        108|     Nancy|  2002-08-17|2002-08-19|\n","|        109|    Daniel|  2002-08-16|2002-08-19|\n","|        110|      John|  2005-09-28|2005-10-03|\n","+-----------+----------+------------+----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import next_day\n","\n","proj_kt_df = df_joining_date.withColumn(\"PROJECT_KT\", \\\n","                                        next_day(df_joining_date.JOINING_DATE, \"Monday\"))\n","\n","proj_kt_df.show()"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"8OHVhKDuGuy7"},"outputs":[],"source":["#THIS EMPTY CELL IS LEFT INTENTIONALLY"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","---\n","\n","\n","    \n","    \n","    Thank you. I hope this notebook helps you!"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOoWF9DWWCv2v1cO+l7froU","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4 (main, Jan 25 2023, 00:13:50) [GCC 9.4.0]"},"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":0}
